{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n-----------------------------------------------------------------------------\\n\\n                   Spark with Python             \\n                    \\n                 Dataframe & Spark SQL\\n-----------------------------------------------------------------------------\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "DATAFRAMES\n",
    "São coleções distribuídas de dados agrupados em colunas nomeadas. Ou seja, são estruturados (ao contrário dos RDDS) - como se fossem uma coleção de tabelas.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "SpSession = SparkSession.builder.appName(\"Trabalhando com Data Frames\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----------------+------+\n",
      "|age|deptid|gender|             name|salary|\n",
      "+---+------+------+-----------------+------+\n",
      "| 32|   100|  male|Benjamin Garrison|  3000|\n",
      "| 40|   200|  male|    Holland Drake|  4500|\n",
      "| 26|   100|  male|  Burks Velasquez|  2700|\n",
      "| 51|   100|female|    June Rutledge|  4300|\n",
      "| 44|   200|  male|    Nielsen Knapp|  6500|\n",
      "+---+------+------+-----------------+------+\n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- deptid: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Cria um dataframe a partir de um arquivo json\n",
    "empDf = SpSession.read.json(\"customerData.json\")\n",
    "empDf.show()\n",
    "empDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|Benjamin Garrison|\n",
      "|    Holland Drake|\n",
      "|  Burks Velasquez|\n",
      "|    June Rutledge|\n",
      "|    Nielsen Knapp|\n",
      "+-----------------+\n",
      "\n",
      "+---+------+------+-------------+------+\n",
      "|age|deptid|gender|         name|salary|\n",
      "+---+------+------+-------------+------+\n",
      "| 40|   200|  male|Holland Drake|  4500|\n",
      "+---+------+------+-------------+------+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|female|    1|\n",
      "|  male|    4|\n",
      "+------+-----+\n",
      "\n",
      "+------+------------------+--------+\n",
      "|deptid|       avg(salary)|max(age)|\n",
      "+------+------------------+--------+\n",
      "|   200|            5500.0|      44|\n",
      "|   100|3333.3333333333335|      51|\n",
      "+------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Consulta o data frame\n",
    "empDf.select(\"name\").show()\n",
    "empDf.filter(empDf[\"age\"] == 40).show()\n",
    "empDf.groupBy(\"gender\").count().show()\n",
    "empDf.groupBy(\"deptid\").\\\n",
    "    agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|100|      Sales|\n",
      "|200|Engineering|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cria um data frame a partir de uma lista\n",
    "deptList = [{'name': 'Sales', 'id': \"100\"},\\\n",
    " { 'name':'Engineering','id':\"200\" }]\n",
    "deptDf = SpSession.createDataFrame(deptList)\n",
    "deptDf.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----------------+------+---+-----------+\n",
      "|age|deptid|gender|             name|salary| id|       name|\n",
      "+---+------+------+-----------------+------+---+-----------+\n",
      "| 51|   100|female|    June Rutledge|  4300|100|      Sales|\n",
      "| 26|   100|  male|  Burks Velasquez|  2700|100|      Sales|\n",
      "| 32|   100|  male|Benjamin Garrison|  3000|100|      Sales|\n",
      "| 44|   200|  male|    Nielsen Knapp|  6500|200|Engineering|\n",
      "| 40|   200|  male|    Holland Drake|  4500|200|Engineering|\n",
      "+---+------+------+-----------------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#junção de data frames\n",
    "empDf.join(deptDf, empDf.deptid == deptDf.id).show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+\n",
      "|deptid|avg(salary)|max(age)|\n",
      "+------+-----------+--------+\n",
      "|   200|     5500.0|      44|\n",
      "|   100|     3650.0|      51|\n",
      "+------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#operações em cascata\n",
    "empDf.filter(empDf[\"age\"] >30).join(deptDf, \\\n",
    "        empDf.deptid == deptDf.id).\\\n",
    "        groupBy(\"deptid\").\\\n",
    "        agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#............................................................................\n",
    "##   Criando dataframes a partir de um rdd\n",
    "#............................................................................\n",
    "\n",
    "#Inicializa SparkSession e SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#Cria a Spark Session\n",
    "SpSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"Dataframe a partir de um RDD & SQL\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.cores.max\",\"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///c:/temp/spark-warehouse\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Obtém  a Spark Context do Spark Session    \n",
    "SpContext = SpSession.sparkContext\n",
    "\n",
    "lines = SpContext.textFile(\"auto-data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove a primeira linha\n",
    "datalines = lines.filter(lambda x: \"FUELTYPE\" not in x)\n",
    "datalines.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = datalines.map(lambda l: l.split(\",\"))\n",
    "autoMap = parts.map(lambda p: Row(make=p[0],\\\n",
    "         body=p[4], hp=int(p[7])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|      make|     body| hp|\n",
      "+----------+---------+---+\n",
      "|    subaru|hatchback| 69|\n",
      "| chevrolet|hatchback| 48|\n",
      "|     mazda|hatchback| 68|\n",
      "|    toyota|hatchback| 62|\n",
      "|mitsubishi|hatchback| 68|\n",
      "|     honda|hatchback| 60|\n",
      "|    nissan|    sedan| 69|\n",
      "|     dodge|hatchback| 68|\n",
      "|  plymouth|hatchback| 68|\n",
      "|     mazda|hatchback| 68|\n",
      "|mitsubishi|hatchback| 68|\n",
      "|     dodge|hatchback| 68|\n",
      "|  plymouth|hatchback| 68|\n",
      "| chevrolet|hatchback| 70|\n",
      "|    toyota|hatchback| 62|\n",
      "|     dodge|hatchback| 68|\n",
      "|     honda|hatchback| 58|\n",
      "|    toyota|hatchback| 62|\n",
      "|     honda|hatchback| 76|\n",
      "| chevrolet|    sedan| 70|\n",
      "+----------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registra o dataframe como tabela\n",
    "autoDf = SpSession.createDataFrame(autoMap)\n",
    "autoDf.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "|      MAKE|FUELTYPE|ASPIRE|DOORS|     BODY|DRIVE|CYLINDERS| HP| RPM|MPG-CITY|MPG-HWY|PRICE|\n",
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "|    subaru|     gas|   std|  two|hatchback|  fwd|     four| 69|4900|      31|     36| 5118|\n",
      "| chevrolet|     gas|   std|  two|hatchback|  fwd|    three| 48|5100|      47|     53| 5151|\n",
      "|     mazda|     gas|   std|  two|hatchback|  fwd|     four| 68|5000|      30|     31| 5195|\n",
      "|    toyota|     gas|   std|  two|hatchback|  fwd|     four| 62|4800|      35|     39| 5348|\n",
      "|mitsubishi|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5389|\n",
      "|     honda|     gas|   std|  two|hatchback|  fwd|     four| 60|5500|      38|     42| 5399|\n",
      "|    nissan|     gas|   std|  two|    sedan|  fwd|     four| 69|5200|      31|     37| 5499|\n",
      "|     dodge|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5572|\n",
      "|  plymouth|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5572|\n",
      "|     mazda|     gas|   std|  two|hatchback|  fwd|     four| 68|5000|      31|     38| 6095|\n",
      "|mitsubishi|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      31|     38| 6189|\n",
      "|     dodge|     gas|   std| four|hatchback|  fwd|     four| 68|5500|      31|     38| 6229|\n",
      "|  plymouth|     gas|   std| four|hatchback|  fwd|     four| 68|5500|      31|     38| 6229|\n",
      "| chevrolet|     gas|   std|  two|hatchback|  fwd|     four| 70|5400|      38|     43| 6295|\n",
      "|    toyota|     gas|   std|  two|hatchback|  fwd|     four| 62|4800|      31|     38| 6338|\n",
      "|     dodge|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      31|     38| 6377|\n",
      "|     honda|     gas|   std|  two|hatchback|  fwd|     four| 58|4800|      49|     54| 6479|\n",
      "|    toyota|     gas|   std| four|hatchback|  fwd|     four| 62|4800|      31|     38| 6488|\n",
      "|     honda|     gas|   std|  two|hatchback|  fwd|     four| 76|6000|      30|     34| 6529|\n",
      "| chevrolet|     gas|   std| four|    sedan|  fwd|     four| 70|5400|      38|     43| 6575|\n",
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#............................................................................\n",
    "##   Cria dataframe a partir de um arquivo CSV\n",
    "#...........................................................................\n",
    "autoDf1 = SpSession.read.csv(\"auto-data.csv\",header=True)\n",
    "autoDf1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+\n",
      "|   make|       body| hp|\n",
      "+-------+-----------+---+\n",
      "|porsche|    hardtop|207|\n",
      "|porsche|    hardtop|207|\n",
      "| jaguar|      sedan|262|\n",
      "|porsche|convertible|207|\n",
      "+-------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#............................................................................\n",
    "##   Criando e trabalhando com tabelas temporárias\n",
    "#............................................................................\n",
    "\n",
    "autoDf.createOrReplaceTempView(\"autos\")\n",
    "SpSession.sql(\"select * from autos where hp > 200\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------------+------+\n",
      "|age|deptid|gender|         name|salary|\n",
      "+---+------+------+-------------+------+\n",
      "| 40|   200|  male|Holland Drake|  4500|\n",
      "| 51|   100|female|June Rutledge|  4300|\n",
      "| 44|   200|  male|Nielsen Knapp|  6500|\n",
      "+---+------+------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Registrando um dataframe como tabela e executando comandos SQL\n",
    "empDf.createOrReplaceTempView(\"employees\")\n",
    "SpSession.sql(\"select * from employees where salary > 4000\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "4500\n",
      "2700\n",
      "4300\n",
      "6500\n"
     ]
    }
   ],
   "source": [
    "#Convertendo para um dataframe Pandas\n",
    "empPands = empDf.toPandas()\n",
    "for index, row in empPands.iterrows():\n",
    "    print(row[\"salary\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o254.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:99)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-65c8cc655996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#spark-defaults.conf file to include the driver files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m demoDf = SpSession.read.format(\"jdbc\").options(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"jdbc:mysql://localhost:3306/demo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"com.mysql.jdbc.Driver\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o254.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:99)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "#............................................................................\n",
    "##   Trabalhando com Banco de Dados\n",
    "\n",
    "#Importe a classe que conecta com o seu banco de dados aqui\n",
    "#import classeConectaSgdbXpto \n",
    "# ou\n",
    "\n",
    "#............................................................................\n",
    "#Certifique-se de que o spark classpaths estão setados corretamente no \n",
    "#arquivo spark-defaults.conf para incluir os arquivos de driver\n",
    "    \n",
    "demoDf = SpSession.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://localhost:3306/demo\",\n",
    "    driver = \"com.mysql.jdbc.Driver\",\n",
    "    dbtable = \"demotable\",\n",
    "    user=\"root\",\n",
    "    password=\"\").load()\n",
    "    \n",
    "demoDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
